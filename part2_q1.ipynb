import numpy as np
import random

def q_learning(env, num_episodes, max_steps, learning_rate, discount_factor, epsilon):
    # Initialize the Q-table
    q_table = np.zeros((env.observation_space.shape[0], env.action_space.n))

    # Iterate over episodes
    for episode in range(num_episodes):
        # Reset the environment for each episode
        state = env.reset()

        # Iterate over steps within each episode
        for step in range(max_steps):
            # Choose an action based on epsilon-greedy policy
            if random.random() < epsilon:
                action = env.action_space.sample()
            else:
                action = np.argmax(q_table[state])

            # Take the chosen action and observe the next state and reward
            next_state, reward, done, _ = env.step(action)

            # Update the Q-table using the Q-learning update rule
            q_table[state][action] += learning_rate * (reward + discount_factor * np.max(q_table[next_state]) - q_table[state][action])

            # Move to the next state
            state = next_state

            # Break if the episode is finished
            if done:
                break

    return q_table
def main():
    # Create the CartPole environment
    env = gym.make('CartPole-v1')

    # Set hyperparameters
    num_episodes = 1000
    max_steps = 200
    learning_rate = 0.1
    discount_factor = 0.99
    epsilon = 0.1

    # Run the Q-learning algorithm
    q_table = q_learning(env, num_episodes, max_steps, learning_rate, discount_factor, epsilon)

    # Print the learned Q-table
    print("Learned Q-table:")
    print(q_table)

if __name__ == "__main__":
    main()
